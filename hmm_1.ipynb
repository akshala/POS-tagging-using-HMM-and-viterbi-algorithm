{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hmm_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWng7UhoA4i3dYR3FKOA4F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshala/POS-tagging-using-HMM-and-viterbi-algorithm/blob/main/hmm_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjGrf28D3P4M",
        "outputId": "ce0749b6-684d-4ed8-f2e0-1882d505e01e",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from google.colab import files   # upload Brown_train.txt\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-459ac50a-6668-412e-b5fd-5ddf7d630cec\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-459ac50a-6668-412e-b5fd-5ddf7d630cec\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Brown_train.txt to Brown_train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGMXlFeMJkTo"
      },
      "source": [
        "from statistics import mean\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statistics import harmonic_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eHESIPb4CJ0"
      },
      "source": [
        "with open('Brown_train.txt') as f:    # read file\n",
        "  text = f.read()\n",
        "# text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaFSisOOD27B"
      },
      "source": [
        "sentences = text.split('\\n')     # split into sentences\n",
        "sentences = ['<s>_<s> ' + elt for elt in sentences if len(elt) > 0]     # add start token\n",
        "# sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_ERP0rhzVPR",
        "outputId": "dd525d7f-d50f-4769-daa6-f6e3b700a2b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "final = []   # list of sentences, where sentences are a list of (word, POS)\n",
        "all_words = []   # list of all words\n",
        "for sentence in sentences:\n",
        "  if len(sentence) == 0:   # if length of sentence is 0, discard\n",
        "    continue\n",
        "  words = sentence.split()   # split sentence into words\n",
        "  all_words.extend(words)   \n",
        "  word_tag_pair = []\n",
        "  for word in words:\n",
        "    word = word.split('_')   # split word and POS tag\n",
        "    try:\n",
        "      word_tag_pair.append((word[0], word[1]))   # append tupple\n",
        "    except IndexError:\n",
        "      pass\n",
        "  final.append(word_tag_pair)   # tuples of a sentence to list\n",
        "# final\n",
        "len(all_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1216346"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22U3i2HuEZaK",
        "outputId": "3358de7a-8243-44ed-9c5f-c15ca736887a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "sentence_lengths = []   # list containing length of all sentences\n",
        "for sentence in sentences:\n",
        "  sentence_lengths.append(len(sentence))    # add sentence length\n",
        "print('max sentence length', max(sentence_lengths))\n",
        "print('min sentence length', min(sentence_lengths))\n",
        "print('avg sentence length', mean(sentence_lengths))\n",
        "print('total number of sentences', len(sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max sentence length 1968\n",
            "min sentence length 11\n",
            "avg sentence length 186.82925015867258\n",
            "total number of sentences 55145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr20LzP1HPVh",
        "outputId": "0439c3ba-8053-48b8-f4d3-f443602dc50d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "vocab_word_tag = {}      # dictionary with word as key and all possible tags\n",
        "tag_word_dict = {}      # dictionary with tag as key and all possible tags\n",
        "for sentence in final:\n",
        "  for pair in sentence:\n",
        "    word = pair[0].lower()    # lowercase word\n",
        "    tag = pair[1]             # get tag\n",
        "    if word in vocab_word_tag:\n",
        "      vocab_word_tag[word].add(tag)     # add tag in a set for a word\n",
        "    else:\n",
        "      vocab_word_tag[word] = set()\n",
        "      vocab_word_tag[word].add(tag)      # add tag in a set for a word\n",
        "    if tag in tag_word_dict:\n",
        "      tag_word_dict[tag].add(word)      # add word in a set for a tag\n",
        "    else:\n",
        "      tag_word_dict[tag] = set()\n",
        "      tag_word_dict[tag].add(word)      # add word in a set for a tag\n",
        "print('word key tag value', len(vocab_word_tag))\n",
        "print('tag key word value', len(tag_word_dict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word key tag value 49810\n",
            "tag key word value 473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMi1FefxSD8x",
        "outputId": "ef88d111-0c6a-49bd-dfa2-835bd0229272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = list(vocab_word_tag.keys())    # list of unique words -> vocab\n",
        "vocab.sort()\n",
        "len(vocab)\n",
        "# vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49810"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2uDZxfVUwLK",
        "outputId": "2dd5fdd8-dffe-493c-9eaf-0b16dee1d010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "full_tags = list(tag_word_dict.keys())     # list of unique tags\n",
        "full_tags.sort()\n",
        "len(full_tags)\n",
        "# tags"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "473"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNBXWy8ff5Be"
      },
      "source": [
        "def get_tag_vocab(input_array):\n",
        "  vocab_word_tag = {}        # dictionary with word as key and all possible tags\n",
        "  tag_word_dict = {}         # dictionary with tag as key and all possible tags\n",
        "  for sentence in input_array:\n",
        "    for pair in sentence:\n",
        "      word = pair[0].lower()    # lowercase word\n",
        "      tag = pair[1]             # get tag\n",
        "      if word in vocab_word_tag:\n",
        "        vocab_word_tag[word].add(tag)    # add tag in a set for a word\n",
        "      else:\n",
        "        vocab_word_tag[word] = set()\n",
        "        vocab_word_tag[word].add(tag)     # add tag in a set for a word\n",
        "      if tag in tag_word_dict:\n",
        "        tag_word_dict[tag].add(word)       # add word in a set for a tag\n",
        "      else:\n",
        "        tag_word_dict[tag] = set()\n",
        "        tag_word_dict[tag].add(word)      # add word in a set for a tag\n",
        "  vocab = list(vocab_word_tag.keys())     # list of unique words -> vocab\n",
        "  vocab.append('<unknown>')       # add unknown token for unseen words in test data\n",
        "  vocab.sort()\n",
        "  tags = list(tag_word_dict.keys())        # list of unique tags\n",
        "  tags.sort()\n",
        "  vocab_word_tag['<unknown>'] = set()    \n",
        "  for elt in tags:\n",
        "    vocab_word_tag['<unknown>'].add(elt)    # add all possible tags with the unknown token\n",
        "  return vocab, tags, vocab_word_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er_a7mRzwWql"
      },
      "source": [
        "num_words_for_tag = {}     # number of words for each tag\n",
        "for key, val in tag_word_dict.items():\n",
        "  num_words_for_tag[key] = len(val)\n",
        "# num_words_for_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AyNJbD2nPDt",
        "outputId": "93640efb-7c12-4371-98f6-bdc2198802da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "total_words_vocab = vocab_word_tag.keys()    # total number of words in vocab\n",
        "len(total_words_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49810"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM-jT6t8DVaN"
      },
      "source": [
        "def get_counts(input_array):\n",
        "  transition_counts = {}     # transition counts: current tag previous tag\n",
        "  emission_counts = {}       # emission counts: current word cuurent tag\n",
        "  tag_counts = {}            # count of all tags\n",
        "  for sentence in input_array:      # for each sentence\n",
        "    prev_tag = '<s>'         # add start token\n",
        "    for pair in sentence:\n",
        "      word = pair[0].lower()   # lowercase word\n",
        "      tag = pair[1]\n",
        "      if (tag, prev_tag) not in transition_counts:    # increment for occurence of tag, prev tag\n",
        "        transition_counts[(tag, prev_tag)] = 1\n",
        "      else:\n",
        "        transition_counts[(tag, prev_tag)] += 1        # increment for occurence of tag, prev tag\n",
        "      if (word, tag) not in emission_counts:\n",
        "        emission_counts[(word, tag)] = 1             # increment for occurence of word, tag\n",
        "      else:\n",
        "        emission_counts[(word, tag)] += 1            # increment for occurence of word, tag\n",
        "      if tag not in tag_counts:\n",
        "        tag_counts[tag] = 1       # increase tag count\n",
        "      else:\n",
        "        tag_counts[tag] += 1      # increase tag count\n",
        "      prev_tag = tag  \n",
        "  return transition_counts, emission_counts, tag_counts     # return dictionaries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AEJEmn9LjY2"
      },
      "source": [
        "def get_transition_matrix(transition_counts, tag_counts, tags):\n",
        "  transition_prob = {}      # transition probabilities: current tag previous tag\n",
        "  for key, val in transition_counts.items():\n",
        "    prev_tag = key[1]\n",
        "    transition_prob[key] = val/tag_counts[prev_tag]    # count of current tag given previous tag / count of previous tag\n",
        "  return transition_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_x5Svgnlwnb"
      },
      "source": [
        "def get_emission_matrix(emission_counts, tag_counts, tags, vocab):\n",
        "  emission_prob = {}       # emission probabilities: current word, current tag\n",
        "  total_tag_count = sum(list(tag_counts.values()))\n",
        "  for key, val in emission_counts.items():\n",
        "    tag = key[1]\n",
        "    emission_prob[key] = val/tag_counts[tag]    # count of current word given current tag / count of current tag\n",
        "  for key, val in tag_counts.items():\n",
        "    word = '<unknown>'\n",
        "    emission_prob[(word, key)] = val/total_tag_count    # assign emission probability of unknown token using tag frequency\n",
        "  return emission_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVEVzqwnZDTS"
      },
      "source": [
        "def viterbi(transition_probs, emission_probs, test, vocab_word_tag):\n",
        "  prev_word = '<s>'     # start tag\n",
        "  predicted = []        # predicted tags\n",
        "  count = 1\n",
        "  for word in test:\n",
        "    prob = []\n",
        "    prob_tag = []\n",
        "    for prev_tag in vocab_word_tag[prev_word]:     # consider all possible tags for previous word based on train data\n",
        "      try:\n",
        "        tag_list = vocab_word_tag[word]       # get all possible tags of current word based on train data\n",
        "      except KeyError:\n",
        "        tag_list = vocab_word_tag['<unknown>']    # if word not in train set vocab then assign unknown token\n",
        "        word = '<unknown>'\n",
        "      for cur_tag in tag_list:\n",
        "        try:\n",
        "          if word == '<unknown>':\n",
        "            prob.append(np.log(transition_probs[(cur_tag, prev_tag)]))     # if word unknown only take transition probability\n",
        "          else:\n",
        "            prob.append(np.log(transition_probs[(cur_tag, prev_tag)]) + np.log(emission_probs[(word, cur_tag)]))   # otherwise sum of tranisition and emission probability\n",
        "        except KeyError:\n",
        "          prob.append(np.log(emission_probs[(word, cur_tag)]))   # if cur, prev tag combo not present consider only emission probability\n",
        "        prob_tag.append(cur_tag)\n",
        "      prev_word = word\n",
        "    n = len(prob)\n",
        "    max_prob = float('-inf')\n",
        "    max_prob_tag = ''\n",
        "    for i in range(n):\n",
        "      if prob[i] > max_prob:     # find max probability tag\n",
        "        max_prob = prob[i]\n",
        "        max_prob_tag = prob_tag[i]\n",
        "    splitted = max_prob_tag.split('-')\n",
        "    if splitted[0] == 'FW':\n",
        "      predicted.append(splitted[1]) \n",
        "    else: \n",
        "      predicted.append(splitted[0])\n",
        "    # print(count, max_prob_tag)\n",
        "    count += 1\n",
        "  return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSkL6nNZlwvF"
      },
      "source": [
        "def word_tag_seperate(word_tag_pair_sentences):\n",
        "  words = []\n",
        "  tags = []\n",
        "  for sentence in word_tag_pair_sentences:    # separate words and tags in sentences\n",
        "    sentence_words = []\n",
        "    sentences_tags = []\n",
        "    for pair in sentence:\n",
        "      words.append(pair[0].lower())    # lowercase word\n",
        "      tags.append(pair[1])\n",
        "  return words, tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snC-9S6MOZ-A",
        "outputId": "d4cd786b-334c-4edb-db74-75648f119836",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "kFold = KFold(n_splits=3)\n",
        "predicted_tags = []      # predicted tags\n",
        "all_test_tags = []       # actual tags\n",
        "for train, test in kFold.split(final):    # 3 fold split\n",
        "  training_set = []\n",
        "  testing_set = []\n",
        "  for elt in train:\n",
        "    training_set.append(final[elt])     # get train set\n",
        "  for elt in test:\n",
        "    testing_set.append(final[elt])      # get test set\n",
        "  test_words, test_tags = word_tag_seperate(testing_set)      # separate lists for test words and tags\n",
        "  all_test_tags.append(test_tags)\n",
        "  vocab, tags, vocab_word_tag = get_tag_vocab(training_set)    # get train vocab, tags, dictionary\n",
        "  print(len(test_words))\n",
        "\n",
        "  transition_counts, emission_counts, tag_counts = get_counts(training_set)    # transition, emission, tag counts from train set\n",
        "  # print(transition_counts)\n",
        "  # print(emission_counts)\n",
        "  transition_probs = get_transition_matrix(transition_counts, tag_counts, tags)   # transition probability from train set\n",
        "  emission_probs = get_emission_matrix(emission_counts, tag_counts, tags, vocab)  # emission probability from train set\n",
        "  # print(transition_probs)\n",
        "  predicted_tags.append(viterbi(transition_probs, emission_probs, test_words, vocab_word_tag))   # viterbi algorithm\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "446378\n",
            "477594\n",
            "292365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSfkQUs3Op7B"
      },
      "source": [
        "def accuracy(actual, pred):   # calculate accuracy\n",
        "  n = len(actual)\n",
        "  correct = 0\n",
        "  for i in range(n):\n",
        "    if actual[i] == pred[i]:    # if actual=pred increment\n",
        "      correct += 1\n",
        "    # else:\n",
        "    #   print(actual[i], pred[i])\n",
        "  return correct/n     # correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YESxfDswAeK2",
        "outputId": "ce9c1268-7507-4cf4-f633-630586d96c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "for i in range(3):\n",
        "  print(accuracy(all_test_tags[i], predicted_tags[i]))   \n",
        "  # break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8076047654678322\n",
            "0.8475944002646599\n",
            "0.834976826911566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33o200zKCEJ5"
      },
      "source": [
        "def performance_metrics(actual, pred, tags, num):\n",
        "  # print(len(actual), len(pred), len(tags))\n",
        "  df = pd.DataFrame(0, columns = tags, index = tags) # columns are true tags, rows are predicted tags\n",
        "  # print(df)\n",
        "  n = len(actual)  \n",
        "  for i in range(n):\n",
        "    try:\n",
        "      df[actual[i]][pred[i]] += 1      # increment count in confusion matrix\n",
        "    except KeyError:\n",
        "      pass\n",
        "  df.to_csv(str(num) + '_confusion_matrix.csv')    # save to csv\n",
        "  precision = {}\n",
        "  recall = {}\n",
        "  predicted_sum = df.sum(axis=0)     # calculate sum of predicted tags\n",
        "  actual_sum = df.sum(axis=1)        # calculate sum of actual tags\n",
        "  for tag in tags:\n",
        "    precision[tag] = [df[tag][tag]/predicted_sum[tag]]     # tagwise precision\n",
        "    recall[tag] = [df[tag][tag]/actual_sum[tag]]           # tagwise recall\n",
        "  f1 = {}\n",
        "  for tag in tags:\n",
        "    f1[tag] = [harmonic_mean([precision[tag][0], recall[tag][0]])]    # F1 score is HM of precision and recall\n",
        "  # print('Tagwise precision', precision)\n",
        "  # print('Tagwise recall', recall)\n",
        "  # print('Tagwise F1 score', f1)\n",
        "  df_precision = pd.DataFrame.from_dict(precision) \n",
        "  df_recall = pd.DataFrame.from_dict(recall) \n",
        "  df_f1 = pd.DataFrame.from_dict(f1) \n",
        "  df_precision.to_csv(str(num) + '_precision.csv')\n",
        "  df_recall.to_csv(str(num) + '_recall.csv')\n",
        "  df_f1.to_csv(str(num) + '_f1.csv')\n",
        "\n",
        "  overall_precision = df_precision.stack().mean()     # overall precision is average of tagwise\n",
        "  overall_recall = df_recall.stack().mean()           # overall recall is average of tagwise\n",
        "  overall_f1 = df_f1.stack().mean()                   # overall F1 score is average of tagwise\n",
        "\n",
        "  print('Precision', overall_precision)\n",
        "  print('Recall', overall_recall)\n",
        "  print('F1 score', overall_f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7GNGKEFEdNY",
        "outputId": "9154596f-104e-42ee-956d-727de1ee962a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "for i in range(3):\n",
        "  print('Fold', i+1)     # calculate for each fold\n",
        "  performance_metrics(all_test_tags[i], predicted_tags[i], full_tags, i)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in long_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in long_scalars\n",
            "/usr/lib/python3.6/statistics.py:356: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  T, total, count = _sum(1/x for x in _fail_neg(data, errmsg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Precision 0.32152787909545194\n",
            "Recall 0.7389588927962093\n",
            "F1 score 0.757546302812592\n",
            "Fold 2\n",
            "Precision 0.2747432775971587\n",
            "Recall 0.7375803646944107\n",
            "F1 score 0.7902254630792261\n",
            "Fold 3\n",
            "Precision 0.3998570743977475\n",
            "Recall 0.7575858149496483\n",
            "F1 score 0.7536152170715897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhuxEPxWIyAO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}