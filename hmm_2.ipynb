{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hmm_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPv1+vyc+2t8qEQSJx0WOyD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshala/POS-tagging-using-HMM-and-viterbi-algorithm/blob/main/hmm_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjGrf28D3P4M",
        "outputId": "d8e7c247-527c-43f7-a04d-2df9eed334de",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-40ec864c-251c-4b23-a08a-b128ce8c60ed\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-40ec864c-251c-4b23-a08a-b128ce8c60ed\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Brown_train.txt to Brown_train (2).txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZvJNKps6ipN",
        "outputId": "dd1edf30-43a7-410d-f71c-232823f70765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGMXlFeMJkTo"
      },
      "source": [
        "from statistics import mean\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eHESIPb4CJ0"
      },
      "source": [
        "with open('Brown_train.txt') as f:\n",
        "  text = f.read()\n",
        "# text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaFSisOOD27B"
      },
      "source": [
        "sentences = text.split('\\n')\n",
        "sentences = ['<s>_<s> <s>_<s> ' + elt for elt in sentences if len(elt) > 0]\n",
        "# sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_ERP0rhzVPR"
      },
      "source": [
        "final = []\n",
        "all_words = []\n",
        "for sentence in sentences:\n",
        "  if len(sentence) == 0:\n",
        "    continue\n",
        "  words = sentence.split()\n",
        "  all_words.extend(words)\n",
        "  word_tag_pair = []\n",
        "  for word in words:\n",
        "    word = word.split('_')\n",
        "    try:\n",
        "      word_tag_pair.append((word[0], word[1]))\n",
        "    except IndexError:\n",
        "      pass\n",
        "  final.append(word_tag_pair)\n",
        "# final\n",
        "# len(all_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22U3i2HuEZaK"
      },
      "source": [
        "sentence_lengths = []\n",
        "for sentence in sentences:\n",
        "  sentence_lengths.append(len(sentence))\n",
        "# print('max sentence length', max(sentence_lengths))\n",
        "# print('min sentence length', min(sentence_lengths))\n",
        "# print('avg sentence length', mean(sentence_lengths))\n",
        "# print('total number of sentences', len(sentence_lengths))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr20LzP1HPVh"
      },
      "source": [
        "vocab_word_tag = {}\n",
        "tag_word_dict = {}\n",
        "for sentence in final:\n",
        "  for pair in sentence:\n",
        "    word = pair[0].lower()\n",
        "    tag = pair[1]\n",
        "    if word in vocab_word_tag:\n",
        "      vocab_word_tag[word].add(tag)\n",
        "    else:\n",
        "      vocab_word_tag[word] = set()\n",
        "      vocab_word_tag[word].add(tag)\n",
        "    if tag in tag_word_dict:\n",
        "      tag_word_dict[tag].add(word)\n",
        "    else:\n",
        "      tag_word_dict[tag] = set()\n",
        "      tag_word_dict[tag].add(word)\n",
        "# print('word key tag value', len(vocab_word_tag))\n",
        "# print('tag key word value', len(tag_word_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMi1FefxSD8x"
      },
      "source": [
        "vocab = list(vocab_word_tag.keys())\n",
        "vocab.sort()\n",
        "# len(vocab)\n",
        "# vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2uDZxfVUwLK"
      },
      "source": [
        "tags = list(tag_word_dict.keys())\n",
        "tags.sort()\n",
        "# len(tags)\n",
        "# tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNBXWy8ff5Be"
      },
      "source": [
        "def get_tag_vocab(input_array):\n",
        "  vocab_word_tag = {}\n",
        "  tag_word_dict = {}\n",
        "  for sentence in input_array:\n",
        "    for pair in sentence:\n",
        "      word = pair[0].lower()\n",
        "      tag = pair[1]\n",
        "      if word in vocab_word_tag:\n",
        "        vocab_word_tag[word].add(tag)\n",
        "      else:\n",
        "        vocab_word_tag[word] = set()\n",
        "        vocab_word_tag[word].add(tag)\n",
        "      if tag in tag_word_dict:\n",
        "        tag_word_dict[tag].add(word)\n",
        "      else:\n",
        "        tag_word_dict[tag] = set()\n",
        "        tag_word_dict[tag].add(word)\n",
        "  vocab = list(vocab_word_tag.keys())\n",
        "  vocab.append('<unknown>')\n",
        "  vocab.sort()\n",
        "  tags = list(tag_word_dict.keys())\n",
        "  tags.sort()\n",
        "  vocab_word_tag['<unknown>'] = set()\n",
        "  for elt in tags:\n",
        "    vocab_word_tag['<unknown>'].add(elt)\n",
        "  return vocab, tags, vocab_word_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er_a7mRzwWql"
      },
      "source": [
        "num_words_for_tag = {}\n",
        "for key, val in tag_word_dict.items():\n",
        "  num_words_for_tag[key] = len(val)\n",
        "# num_words_for_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AyNJbD2nPDt"
      },
      "source": [
        "total_words_vocab = vocab_word_tag.keys()\n",
        "# len(total_words_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM-jT6t8DVaN"
      },
      "source": [
        "def get_counts(input_array):\n",
        "  transition_counts = {}\n",
        "  emission_counts = {}\n",
        "  tag_counts = {}\n",
        "  for sentence in input_array:\n",
        "    prev1_tag = '<s>'\n",
        "    prev2_tag = '<s>'\n",
        "    for pair in sentence:\n",
        "      word = pair[0].lower()\n",
        "      tag = pair[1]\n",
        "      if (tag, prev1_tag, prev2_tag) not in transition_counts:\n",
        "        transition_counts[(tag, prev1_tag, prev2_tag)] = 1\n",
        "      else:\n",
        "        transition_counts[(tag, prev1_tag, prev2_tag)] += 1\n",
        "      if (word, tag) not in emission_counts:\n",
        "        emission_counts[(word, tag)] = 1\n",
        "      else:\n",
        "        emission_counts[(word, tag)] += 1\n",
        "      if tag not in tag_counts:\n",
        "        tag_counts[tag] = 1\n",
        "      else:\n",
        "        tag_counts[tag] += 1\n",
        "      prev2_tag = prev1_tag\n",
        "      prev1_tag = tag\n",
        "  tag_counts['<unknown>'] = 0\n",
        "  return transition_counts, emission_counts, tag_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVQ4On2EkB2Q"
      },
      "source": [
        "def get_2_tag_counts(input_array):\n",
        "  two_tag_counts = {}\n",
        "  for sentence in input_array:\n",
        "    prev = '<s>'\n",
        "    i = 0\n",
        "    for pair in sentence:\n",
        "      tag = pair[1]\n",
        "      if i != 0:\n",
        "        if (tag, prev) not in two_tag_counts:\n",
        "          two_tag_counts[(tag, prev)] = 1\n",
        "        else:\n",
        "          two_tag_counts[(tag, prev)] += 1\n",
        "      i += 1\n",
        "      prev = tag\n",
        "  return two_tag_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AEJEmn9LjY2"
      },
      "source": [
        "def get_transition_matrix(transition_counts, two_tag_counts, tags):\n",
        "  transition_prob = {}\n",
        "  for key, val in transition_counts.items():\n",
        "    prev1_tag = key[1]\n",
        "    prev2_tag = key[2]\n",
        "    transition_prob[key] = val/two_tag_counts[(prev1_tag, prev2_tag)]\n",
        "  return transition_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_x5Svgnlwnb"
      },
      "source": [
        "def get_emission_matrix(emission_counts, tag_counts, tags, vocab):\n",
        "  emission_prob = {}\n",
        "  total_tag_count = sum(list(tag_counts.values()))\n",
        "  for key, val in emission_counts.items():\n",
        "    tag = key[1]\n",
        "    emission_prob[key] = val/tag_counts[tag]\n",
        "  for key, val in tag_counts.items():\n",
        "    word = '<unknown>'\n",
        "    emission_prob[(word, key)] = val/total_tag_count\n",
        "  return emission_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVEVzqwnZDTS"
      },
      "source": [
        "def viterbi(transition_probs, emission_probs, test, vocab_word_tag):\n",
        "  prev1_word = '<s>'\n",
        "  prev2_word = '<s>'\n",
        "  predicted = []\n",
        "  count = 1\n",
        "  for word in test:\n",
        "    prob = []\n",
        "    prob_tag = []\n",
        "    for prev1_tag in vocab_word_tag[prev1_word]:\n",
        "      for prev2_tag in vocab_word_tag[prev2_word]:\n",
        "        try:\n",
        "          tag_list = vocab_word_tag[word]\n",
        "        except KeyError:\n",
        "          tag_list = vocab_word_tag['<unknown>']\n",
        "          word = '<unknown>'\n",
        "        for cur_tag in tag_list:\n",
        "          try:\n",
        "            if word == '<unknown>':\n",
        "              prob.append(np.log(transition_probs[(cur_tag, prev1_tag, prev2_tag)]))\n",
        "            else:\n",
        "              prob.append(np.log(transition_probs[(cur_tag, prev1_tag, prev2_tag)]) + np.log(emission_probs[(word, cur_tag)]))\n",
        "          except KeyError:\n",
        "            prob.append(np.log(emission_probs[(word, cur_tag)]))\n",
        "          prob_tag.append(cur_tag)\n",
        "      prev2_word = prev1_word\n",
        "      prev1_word = word\n",
        "    n = len(prob)\n",
        "    max_prob = float('-inf')\n",
        "    max_prob_tag = ''\n",
        "    for i in range(n):\n",
        "      if prob[i] > max_prob:\n",
        "        max_prob = prob[i]\n",
        "        max_prob_tag = prob_tag[i]\n",
        "      splitted = max_prob_tag.split('-')\n",
        "      if splitted[0] == 'FW':\n",
        "        predicted.append(splitted[1]) \n",
        "      else: \n",
        "        predicted.append(splitted[0])\n",
        "    # print(count, max_prob_tag)\n",
        "    count += 1\n",
        "  return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSkL6nNZlwvF"
      },
      "source": [
        "def word_tag_seperate(word_tag_pair_sentences):\n",
        "  words = []\n",
        "  tags = []\n",
        "  for sentence in word_tag_pair_sentences:\n",
        "    sentence_words = []\n",
        "    sentences_tags = []\n",
        "    for pair in sentence:\n",
        "      words.append(pair[0].lower())\n",
        "      tags.append(pair[1])\n",
        "  return words, tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snC-9S6MOZ-A",
        "outputId": "6713af3f-0f7c-4f6a-98cf-b07f8ee62fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "kFold = KFold(n_splits=3)\n",
        "predicted_tags = []\n",
        "all_test_tags = []\n",
        "for train, test in kFold.split(final):\n",
        "  training_set = []\n",
        "  testing_set = []\n",
        "  for elt in train:\n",
        "    training_set.append(final[elt])\n",
        "  for elt in test:\n",
        "    testing_set.append(final[elt])\n",
        "  test_words, test_tags = word_tag_seperate(testing_set)\n",
        "  all_test_tags.append(test_tags)\n",
        "  vocab, tags, vocab_word_tag = get_tag_vocab(training_set)\n",
        "  print(len(test_words))\n",
        "\n",
        "  transition_counts, emission_counts, tag_counts = get_counts(training_set)\n",
        "  two_tag_counts = get_2_tag_counts(training_set)\n",
        "  print(two_tag_counts)\n",
        "  print(transition_counts)\n",
        "  print(emission_counts)\n",
        "  transition_probs = get_transition_matrix(transition_counts, two_tag_counts, tags)\n",
        "  emission_probs = get_emission_matrix(emission_counts, tag_counts, tags, vocab)\n",
        "  print(transition_probs)\n",
        "  print(emission_probs)\n",
        "  predicted_tags.append(viterbi(transition_probs, emission_probs, test_words, vocab_word_tag))\n",
        "  # break"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f4f037f02b3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkFold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredicted_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_test_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkFold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtraining_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'KFold' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSfkQUs3Op7B"
      },
      "source": [
        "def accuracy(actual, pred):\n",
        "  n = len(actual)\n",
        "  correct = 0\n",
        "  for i in range(n):\n",
        "    if actual[i] == pred[i]:\n",
        "      correct += 1\n",
        "    # else:\n",
        "    #   print(actual[i], pred[i])\n",
        "  return correct/n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YESxfDswAeK2"
      },
      "source": [
        "for i in range(3):\n",
        "  print(accuracy(all_test_tags[i], predicted_tags[i]))\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsn8XPD44FfS"
      },
      "source": [
        "save_all_test_tags = np.array(all_test_tags)\n",
        "save_predicted_tags = np.array(predicted_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mYMVe7D4PNH"
      },
      "source": [
        "np.save('save_all_test_tags', save_all_test_tags)\n",
        "np.save('save_predicted_tags', save_predicted_tags)\n",
        "# np.save('save_transition_counts', save_transition_counts)\n",
        "# np.save('save_emission_counts', save_emission_counts)\n",
        "# np.save('save_tag_counts', save_tag_counts)\n",
        "# np.save('save_two_tag_counts', save_two_tag_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70m6xdKk0mpI"
      },
      "source": [
        "!cp save_all_test_tags.npy \"drive/My Drive/NLP\"\n",
        "!cp save_predicted_tags.npy \"drive/My Drive/NLP\"\n",
        "# !cp save_transition_counts.npy \"drive/My Drive/NLP\"\n",
        "# !cp save_emission_counts.npy \"drive/My Drive/NLP\"\n",
        "# !cp save_tag_counts.npy \"drive/My Drive/NLP\"\n",
        "# !cp save_two_tag_counts.npy \"drive/My Drive/NLP\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKRA94Yo1FBr"
      },
      "source": [
        "TEST_TAG = np.load('drive/My Drive/NLP/save_all_test_tags.npy')\n",
        "PRED_TAG = np.load('drive/My Drive/NLP/save_all_test_tags.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt_EXSFp7Qby"
      },
      "source": [
        "def performance_metrics(actual, pred, tags, num):\n",
        "  # print(len(actual), len(pred), len(tags))\n",
        "  df = pd.DataFrame(0, columns = tags, index = tags) # columns are true tags, rows are predicted tags\n",
        "  # print(df)\n",
        "  n = len(actual)\n",
        "  for i in range(n):\n",
        "    try:\n",
        "      df[actual[i]][pred[i]] += 1\n",
        "    except KeyError:\n",
        "      pass\n",
        "    # if i%100==0:\n",
        "    #   print('Processed {}'.format(i))\n",
        "  # print('Confusion Matrix')\n",
        "  # print(df)\n",
        "  df.to_csv(str(num) + '_confusion_matrix.csv')\n",
        "  precision = {}\n",
        "  recall = {}\n",
        "  predicted_sum = df.sum(axis=0)\n",
        "  actual_sum = df.sum(axis=1)\n",
        "  for tag in tags:\n",
        "    precision[tag] = [df[tag][tag]/predicted_sum[tag]]\n",
        "    recall[tag] = [df[tag][tag]/actual_sum[tag]]\n",
        "  f1 = {}\n",
        "  for tag in tags:\n",
        "    f1[tag] = [harmonic_mean([precision[tag][0], recall[tag][0]])]\n",
        "  # print('Tagwise precision', precision)\n",
        "  # print('Tagwise recall', recall)\n",
        "  # print('Tagwise F1 score', f1)\n",
        "  df_precision = pd.DataFrame.from_dict(precision) \n",
        "  df_recall = pd.DataFrame.from_dict(recall) \n",
        "  df_f1 = pd.DataFrame.from_dict(f1) \n",
        "  df_precision.to_csv(str(num) + '_precision.csv')\n",
        "  df_recall.to_csv(str(num) + '_recall.csv')\n",
        "  df_f1.to_csv(str(num) + '_f1.csv')\n",
        "\n",
        "  overall_precision = df_precision.stack().mean()\n",
        "  overall_recall = df_recall.stack().mean()\n",
        "  overall_f1 = df_f1.stack().mean()\n",
        "\n",
        "  print('Precision', overall_precision)\n",
        "  print('Recall', overall_recall)\n",
        "  print('F1 score', overall_f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOMFIXI3Srj_"
      },
      "source": [
        "for i in range(3):\n",
        "  print('Fold', i+1)\n",
        "  performance_metrics(all_test_tags[i], predicted_tags[i], full_tags, i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}